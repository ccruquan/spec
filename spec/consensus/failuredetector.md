*** This is the beginning of an unfinished draft. Don't continue reading! ***

# Failure detector

A failure detector is a process that gets as input a header with some height *h*, connects to different Tendermint full nodes, requests the header of height *h* from them, and then cross-checks the headers and the input header.

There are two forseeable use cases:

1) strengthen the lite client: If a lite client accepts a header *hd* (after performing skipping or sequential verification), it can use the failure detector to probe the system for conflicting header and increase the trust in *hd*. Instead of communicating with a single full node, communicating with several full nodes shall increase the probability to be aware of a fork in case there is one.

2) to support fork accountability: In the case when more than 1/3 of the voting power is held by faulty validators, faulty nodes may generate two conflicting headers for the same height. The goal of the failure detector is to learn about the conflicting headers by probing different full nodes. Once a failure detector has two conflicting headers, these headers are evidence of misbehavior. A natural extension is to use the failure detector within a monitor process (on a full node) that calls the failure detector on a sample (or all) headers (in parallel). If the sample is chosen at random, this adds another level of probabilistic reasoning. If conflicting headers are found, they are evidence that can be used for punishing processes.

In this document we will focus in strengthening the lite client, and leave other uses of the failure detection mechanism (e.g., when run on a full node) to the future.

This document is based on the lite client ADR [77d2651 on Dec 27, 2019]
https://github.com/interchainio/tendermint-rs/blob/e2cb9aca0b95430fca2eac154edddc9588038982/docs/architecture/adr-002-lite-client.md

## Context of this document

The lite client specification is designed for the Tendermint failure model (1/3 assumption). It is safe under this assumption, and live if it can reliably and timely communicate with a correct full node. If this assumption is violated, the lite client can be fooled to trust a header that was not generated by Tendermint consensus.

This specification, the failure detector, is a "second line of defense", in case the 1/3 assumption is violated. Its goal is to collect evidence. However, it is impractical to probe all full nodes. At this time we consider a simple scheme of maintaining an address book of known full nodes from which a small subset (4) are chosen initially to communicate with. More involved book keeping with probabilistic guarantees (such as PEX or Brahms) can be considered at later stages of the project.



The lite client maintains a simple address book:
- Fixed list of full nodes provided in the configuration upon initialization
- Select one full as primary, select 3 as secondary
- The bisector communicates with the primary
- The bisector gets headers from the primary, and stores them in *State*

### Informal Problem statement

Whenever a new header *h* is added to *State*, the failure detector should query the secondaries.

If a header *h'* returned by the secondary *s* is equal to *h* we do nothing. **(Q1: or should we record it in case we find a problem when we get another header for this height from a different secondary??)** Otherwise, that is if $h'$ is
different from *h*, we have a fork. There are several cases to distinguish

   - **C1.** *h'* is malformed (fails basic validation): *s* is faulty
   - **C2.** otherwise, the failure detector tries to verify *h'* by bisection

        - **C2F.** *h'* can be verified: there is a fork on the main blockchain
        - **C2L.** otherwise: ? << is this lunatic >>

**Q2: Is C1 the only case where we can mark a secondary faulty?**

**Q3: Should the failure detector analyze the headers, e.g., look for double signing? Or should it just report both headers.**

We have the following requirements

- Any peers that errored should be marked bad

- Update primary and secondaries as necessary (i.e. when any are marked bad)

- If there's a verified conflict, persist it, log the error, attempt to broadcast (if Tendermint supports it ...)

**Q4: If there is a fork, what should the lite client as a whole do? Just issue a warning to the user?**



### Evidence processing

How the generated evidence is used, will be discussed in another document. One solution would be that the failure detector tries to get the evidence accepted on the main chain, which may work in practical scenarios, e.g., if there was no attack on the main chain (agreement of the validators is never violated), but there was an attack on a light client.

 In theory, however,
given that the 1/3 assumption is necessarily violated to violate agreement, there are possible complications to submitting the evidence to the chain:

- evidence might be censored (faulty validators have enough votes to prevent deciding on a block that contains the evidence)
- the faulty processes block all progress. The chain comes to a halt.
- the faulty processes actually generated a fork and both branches make progress (with different correct processes participating in different branches). In this case, none of the branches is "more correct" than the other one. We necessarily fall back to social consensus.

Due to these complications, in the worst case we may need to fall back to social consensus. Also for this, the evidence of the fault detector will be crucial to figure out who misbehaved/what went wrong. In this sense, this specification can be studied independently of the actual fork accountability and punishment scheme.


## Problem statement

From the ADR.
The detection module is for checking if any of the backup nodes are reporting conflicting information. It requests headers from each backup node and compares them with a verified header from the primary. If there is a conflict, it attempts to verify the conflicting header via the verifier. If it can be verified, it indicates an attack on the light clients that should be punishable. The relevant information (ie. the two conflicting commits) are passed to the publisher.


The failure detector is part of the lite client. With the other components (the Bisection), it shares
  - an address book, and
  - the state, which contains trusted headers.

  State
  The light node state contains the following:

  current height (H) - height for the next header we want to verify
  last header (H-1) - the last header we verified
  current validators (H) - validators for the height we want to verify (including all validator pubkeys and voting powers)
  It also includes some configuration, which contains:

  trusting period
  initial list of full nodes
  method (sequential or skipping)
  trust level (if method==skipping)


  Whenever in the process of bisection a new header *hd* is trusted

detector and time

- We assume that *hd.bfttime < now - TRUSTED_PERIOD*

**safety liveness**



## Assumptions/Incentives

It is not in the interest of faulty full nodes to talk to the failure detector. This would only increase the likelihood of misbehavior being detected. Also we cannot punish them easily (cheaply). The absence of a response need not be the fault of the full node. Also, a faulty failure detector could wrongly accuse correct full nodes.

Correct full nodes have the incentive to respond, because the failure detector may help them to understand whether their header is a good one. We can thus base liveness arguments of the failure detector on the assumptions that correct full nodes reliably talk to us.


- Q2: what is the most suitable way of modeling the system for probabilistic guarantees? Possible options from the literature:

  - Strong adversaries (that have a strategy in picking message delays to play against you, in combination with local coin tosses at the level of the failure detector)

  - Randomized schedulers: Roughly, a scheduler randomly decides which process does the next step and which messages are delivered to this process in this step.

  - perhaps just do a synchronous model: in step i the failure detector queries a full node, in step i+1 it does the computation based on the response (including deciding who to query next).

  - We have to understand how the blockchain grows while we do failure detection. How should the failure detector keep up?

-Q3: Are we limiting ourselves to the scenario of too many faults to ensure consensus agreement but not complete takeover, that is, 1/3 n <= f <= 2/3 n?


## Definitions


- The failure detector maintains a set *peers* of identifiers of full nodes. This set is regularly updated by querying a seed node.

-

In case there are two conflicting header for the same height, that is, there is a tuple *t = (_, th, true, _)* in the pool and a newly downloaded header *h* that are different but *th.height = h.height)*, then *t* and *h* shall be added to a set *evidence*.

## Specification

## Solution
